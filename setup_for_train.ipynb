{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f6a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218aae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os\n",
    "os.chdir('/content/drive/MyDrive/sm-embedder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ace03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dataset_splitter import split_dataset \n",
    "from preprocessing import \\\n",
    "        iterator_from_directory, \\\n",
    "        write_images_tfrecord, \\\n",
    "        images_dataset_from_tfrecord\n",
    "from extract_features import \\\n",
    "        load_model as load_backbone, \\\n",
    "        prepare_dataset_for_inference, \\\n",
    "        write_featuremaps_to_tfrecord, \\\n",
    "        featuremaps_dataset_from_tfrecord\n",
    "from mlutils import \\\n",
    "        plot_augmented_images_dataset_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e1bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/home/duo/datasets/desafio/\"\n",
    "build_dir = \"/home/duo/sm-embedder/build\"\n",
    "dataset_splits_dir = os.path.join(build_dir, \"datasets\")\n",
    "input_images_target_size = (300, 300)\n",
    "input_images_target_shape= (300, 300, 3)\n",
    "\n",
    "# Create base directories for artefacts if they don't exist\n",
    "if not os.path.isdir(build_dir):\n",
    "    os.makedirs(build_dir)\n",
    "if not os.path.isdir(dataset_splits_dir):\n",
    "    os.makedirs(dataset_splits_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dffc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the original dataset into 3 subdirectories for training, validation\n",
    "# and testing, preserving the original dataset intact. \n",
    "split_dataset(dataset_dir, dataset_splits_dir, (.8, .1, .1), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This first iterator yields augmented images that will be used for training.\n",
    "train_it = iterator_from_directory(\n",
    "        input_dir=os.path.join(dataset_splits_dir, 'train'),\n",
    "        batch_size=64,\n",
    "        augment=True,\n",
    "        target_size=input_images_target_size)\n",
    "\n",
    "# Writes a tfrecord containing the augmented images and their labels.\n",
    "# I'm using tfrecord for performance and to facilitate dataset manipulation\n",
    "# in the next steps.\n",
    "write_images_tfrecord(\n",
    "        record_fname=os.path.join(build_dir, \"train.tfrecord\"), \n",
    "        iterator=train_it,\n",
    "        n_batches=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving hashes from directory names to a file, to later recover class names \n",
    "with open(os.path.join(build_dir, 'labels.tsv'), 'w') as f:\n",
    "    f.write('\\n'.join(train_it.class_indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e187bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For validation and test datasets, we dont need augmentation. Also, we need\n",
    "# to know how many files we are going to process so we don't repeat images. \n",
    "for subset in ['val', 'test']:\n",
    "    dir_path = os.path.join(dataset_splits_dir, subset)\n",
    "\n",
    "    # Recursively count files inside subset directories\n",
    "    n_files = sum([len(files) for r, d, files in os.walk(dir_path)])\n",
    "\n",
    "    # No augmentation\n",
    "    subset_it = iterator_from_directory(\n",
    "            input_dir=dir_path,\n",
    "            batch_size=n_files,\n",
    "            target_size=input_images_target_size)\n",
    "\n",
    "    # Only one batch, with all files in it\n",
    "    write_images_tfrecord(\n",
    "            record_fname=os.path.join(build_dir, subset+'.tfrecord'),\n",
    "            iterator=subset_it, \n",
    "            n_batches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now here it's a trick to speed up training.\n",
    "# The backbone is not going to be trainable.\n",
    "# Knowing that, we can pre-compute all the outputs for the backbone (from now\n",
    "# on we are going to call them featuremaps), store them on a tfrecord, \n",
    "# and later, feed them to the head of the model during training.\n",
    "# This save A LOT of processor time.\n",
    "# Doing it only for training and validation, because we wanna test the entire\n",
    "# pipeline with the testing dataset.\n",
    "# Go grab a coffe, it'll take some time.\n",
    "for subset in ['train', 'val']:\n",
    "    backbone_model = load_backbone(input_images_target_shape)\n",
    "    images_record = os.path.join(build_dir, subset+'.tfrecord')\n",
    "    featuremaps_record = os.path.join(build_dir, subset+'_featuremaps.tfrecord')\n",
    "\n",
    "    dataset = images_dataset_from_tfrecord(images_record)  \n",
    "    processed_dataset = prepare_dataset_for_inference(dataset) \n",
    "\n",
    "    # Compute the featuremaps and save them\n",
    "    write_featuremaps_to_tfrecord(\n",
    "        featuremaps_record, processed_dataset, backbone_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d0f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting some samples of the augmented images dataset\n",
    "dataset = images_dataset_from_tfrecord(os.path.join(build_dir, 'train.tfrecord')).shuffle(200)\n",
    "plot_augmented_images_dataset_samples(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9279f8",
   "metadata": {},
   "source": [
    "# Next, we are going to train the model on the train.ipynb script. See you there"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
